{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1acb67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T11:57:22.351207Z",
     "iopub.status.busy": "2022-12-01T11:57:22.350636Z",
     "iopub.status.idle": "2022-12-01T11:57:23.264183Z",
     "shell.execute_reply": "2022-12-01T11:57:23.262990Z",
     "shell.execute_reply.started": "2022-12-01T11:57:22.351143Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "import cv2\n",
    "# from utils.utils import cvtColor, preprocess_input\n",
    "import os\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------#\n",
    "#   将图像转换成RGB图像，防止灰度图在预测时报错。\n",
    "#   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n",
    "# ---------------------------------------------------------#\n",
    "def cvtColor(image):\n",
    "    if len(np.shape(image)) == 3 and np.shape(image)[2] == 3:\n",
    "        return image\n",
    "    else:\n",
    "        image = image.convert('RGB')\n",
    "        return image\n",
    "\n",
    "    \n",
    "def preprocess_input(image):\n",
    "    image /= 255.0\n",
    "    return image\n",
    "    \n",
    "    \n",
    "def letterbox_image(image, label , size):\n",
    "    label = Image.fromarray(np.array(label))\n",
    "    '''resize image with unchanged aspect ratio using padding'''\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "    scale = min(w/iw, h/ih)\n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "\n",
    "    image = image.resize((nw,nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128,128,128))\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    label = label.resize((nw,nh), Image.NEAREST)\n",
    "    new_label = Image.new('L', size, (0))\n",
    "    new_label.paste(label, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    return new_image, new_label\n",
    "\n",
    "def rand(a=0, b=1):\n",
    "    return np.random.rand()*(b-a) + a\n",
    "\n",
    "class DeeplabDataset(Dataset):\n",
    "    def __init__(self, annotation_lines, input_shape, num_classes, train, dataset_path):\n",
    "        super(DeeplabDataset, self).__init__()\n",
    "        self.annotation_lines = annotation_lines\n",
    "        self.length = len(annotation_lines)\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.train = train\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annotation_line = self.annotation_lines[index]\n",
    "        name = annotation_line.split()[0]\n",
    "\n",
    "        # -------------------------------#\n",
    "        #   load images\n",
    "        # -------------------------------#\n",
    "        jpg = Image.open(os.path.join(os.path.join(self.dataset_path, \"trainval_image\"), name + \".png\"))\n",
    "        png = Image.open(os.path.join(os.path.join(self.dataset_path, \"trainval_label\"), name + \".png\"))\n",
    "\n",
    "        jpg, png = self.get_random_data(jpg, png, self.input_shape, random=self.train)\n",
    "\n",
    "        jpg = np.transpose(preprocess_input(np.array(jpg, np.float64)), [2, 0, 1])\n",
    "        png = np.array(png)\n",
    "        png[png >= self.num_classes] = self.num_classes\n",
    "        # -------------------------------------------------------#\n",
    "        #   conver to the format of one-hot\n",
    "        # -------------------------------------------------------#\n",
    "        seg_labels = np.eye(self.num_classes + 1)[png.reshape([-1])]\n",
    "        seg_labels = seg_labels.reshape((int(self.input_shape[0]), int(self.input_shape[1]), self.num_classes + 1))\n",
    "\n",
    "        return jpg, png, seg_labels\n",
    "\n",
    "    def rand(self, a=0, b=1):\n",
    "        return np.random.rand() * (b - a) + a\n",
    "\n",
    "    def get_random_data(self, image, label, input_shape, jitter=.3, hue=.1, sat=0.7, val=0.3, random=True):\n",
    "        image = cvtColor(image)\n",
    "        label = Image.fromarray(np.array(label))\n",
    "        # ------------------------------#\n",
    "        #   获得图像的高宽与目标高宽\n",
    "        # ------------------------------#\n",
    "        # iw, ih = image.size\n",
    "        # h, w = input_shape\n",
    "\n",
    "        if not random:\n",
    "            return image, label\n",
    "\n",
    "\n",
    "def deeplab_dataset_collate(batch):\n",
    "    images = []\n",
    "    pngs = []\n",
    "    seg_labels = []\n",
    "    for img, png, labels in batch:\n",
    "        images.append(img)\n",
    "        pngs.append(png)\n",
    "        seg_labels.append(labels)\n",
    "    images = torch.from_numpy(np.array(images)).type(torch.FloatTensor)\n",
    "    pngs = torch.from_numpy(np.array(pngs)).long()\n",
    "    seg_labels = torch.from_numpy(np.array(seg_labels)).type(torch.FloatTensor)\n",
    "    return images, pngs, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa978f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T11:57:23.688099Z",
     "iopub.status.busy": "2022-12-01T11:57:23.687508Z",
     "iopub.status.idle": "2022-12-01T11:57:23.693367Z",
     "shell.execute_reply": "2022-12-01T11:57:23.692025Z",
     "shell.execute_reply.started": "2022-12-01T11:57:23.688032Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from nets.pspnet import PSPNet\n",
    "# from nets.deeplabv3_plus import DeepLab\n",
    "# from load_swin_transformer import SwinUnet as ViT_seg\n",
    "# from Attention_Unet_from_UnetZoo import AttU_Net\n",
    "# from nets.unet import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e792e1ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:02:10.719921Z",
     "iopub.status.busy": "2022-12-01T13:02:10.719344Z",
     "iopub.status.idle": "2022-12-01T13:02:12.045224Z",
     "shell.execute_reply": "2022-12-01T13:02:12.044332Z",
     "shell.execute_reply.started": "2022-12-01T13:02:10.719861Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SwinCAR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class Acon_FReLU(nn.Module):\n",
    "    r\"\"\" ACON activation (activate or not) based on FReLU:\n",
    "    # eta_a(x) = x, eta_b(x) = dw_conv(x), according to\n",
    "    # \"Funnel Activation for Visual Recognition\" <https://arxiv.org/pdf/2007.11824.pdf>.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, width, stride=1):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        # eta_b(x)\n",
    "        self.conv_frelu = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=width, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "\n",
    "        # eta_a(x)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.stride == 2:\n",
    "            x1 = self.maxpool(x)\n",
    "        else:\n",
    "            x1 = x\n",
    "\n",
    "        x2 = self.bn1(self.conv_frelu(x))\n",
    "\n",
    "        return self.bn2((x1 - x2) * self.sigmoid(x1 - x2) + x2)\n",
    "\n",
    "\n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=16):\n",
    "        super(CoordAtt, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp // reduction)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.act = h_swish()  # non-linear\n",
    "\n",
    "        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        n, c, h, w = x.size()\n",
    "        x_h = self.pool_h(x)\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y)\n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        a_h = self.conv_h(x_h).sigmoid()\n",
    "        a_w = self.conv_w(x_w).sigmoid()\n",
    "\n",
    "        out = identity * a_w * a_h\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Recovery_CoordAtt(nn.Module):\n",
    "    def __init__(self, inp1, oup1, reduction=16):\n",
    "        super(Recovery_CoordAtt, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp1 // reduction)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp1, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.act = h_swish()  # non-linear\n",
    "\n",
    "        self.conv = nn.Conv2d(inp1,mip, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.conv1_h = nn.Conv2d(mip, inp1, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv1_w = nn.Conv2d(mip, inp1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.acon = Acon_FReLU(mip, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        # identity: (B,12*C,14,14)\n",
    "        identity = torch.cat((x1,x2),dim=1)\n",
    "\n",
    "        n1, c1, h1, w1 = x1.size()\n",
    "\n",
    "        # x1_h: (B,4*C,14,1)\n",
    "        # x1_w: (B,4*C,14,1)\n",
    "        x1_h = self.pool_h(x1)\n",
    "        x1_w = self.pool_w(x1).permute(0, 1, 3, 2)\n",
    "\n",
    "        # x2_h: (B,8*C,14,1)\n",
    "        # x2_w: (B,8*C,14,1)\n",
    "        x2_h = self.pool_h(x2)\n",
    "        x2_w = self.pool_w(x2).permute(0, 1, 3, 2)\n",
    "\n",
    "        # x_h: (B,12*C,14,1)\n",
    "        x_h = torch.cat((x1_h,x2_h), dim=1)\n",
    "        x_w = torch.cat((x1_w,x2_w), dim=1)\n",
    "\n",
    "        # y: (B,12*C,28,1)\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act(y)\n",
    "\n",
    "\n",
    "        # y_h: (B,12*C/16,14,1)\n",
    "        y_h,y_w = torch.split(y, [h1, w1], dim=2)\n",
    "        y_w = y_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        a_h = self.conv1_h(y_h).sigmoid()\n",
    "        a_w = self.conv1_w(y_w).sigmoid()\n",
    "\n",
    "        out = identity * a_w * a_h\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SignleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double convolution block that keeps that spatial sizes the same\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, norm_layer=None):\n",
    "        super(SignleConv, self).__init__()\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "            norm_layer(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double convolution block that keeps that spatial sizes the same\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, norm_layer=None):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(SignleConv(in_ch, out_ch, norm_layer),\n",
    "                                  SignleConv(out_ch, out_ch, norm_layer))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class FinalUp(nn.Module):\n",
    "    \"\"\"\n",
    "    Doubles spatial size with bilinear upsampling\n",
    "    Skip connections and double convs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(FinalUp, self).__init__()\n",
    "        mode = \"bilinear\"\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: [b,c, h, w]\n",
    "            x2: [b,c, 2*h,2*w]\n",
    "\n",
    "        Returns: 2x upsampled double conv reselt\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.up(x1)\n",
    "\n",
    "        if x2 is not None:\n",
    "            x = torch.cat([x2, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Up1(nn.Module):\n",
    "    \"\"\"\n",
    "    Doubles spatial size with bilinear upsampling\n",
    "    Skip connections and double convs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Up1, self).__init__()\n",
    "        mode = \"bilinear\"\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "        self.recovery_CA = Recovery_CoordAtt(in_ch,in_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: [b,c, h, w]\n",
    "            x2: [b,c, 2*h,2*w]\n",
    "\n",
    "        Returns: 2x upsampled double conv reselt\n",
    "        \"\"\"\n",
    "        x = self.up(x1)\n",
    "\n",
    "        if x2 is not None:\n",
    "            x = torch.cat([x2, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Up2(nn.Module):\n",
    "    \"\"\"\n",
    "    Doubles spatial size with bilinear upsampling\n",
    "    Skip connections and double convs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Up2, self).__init__()\n",
    "        mode = \"bilinear\"\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "        self.conv = SignleConv(in_ch, out_ch)\n",
    "        self.recovery_CA = Recovery_CoordAtt(in_ch,in_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: [b,c, h, w]\n",
    "            x2: [b,c, 2*h,2*w]\n",
    "\n",
    "        Returns: 2x upsampled double conv reselt\n",
    "        \"\"\"\n",
    "        x = self.up(x1)\n",
    "        if x2 is not None:\n",
    "            x = self.recovery_CA(x2,x)\n",
    "            # x = torch.cat([x2, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Up3(nn.Module):\n",
    "    \"\"\"\n",
    "    Doubles spatial size with bilinear upsampling\n",
    "    Skip connections and double convs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Up3, self).__init__()\n",
    "        mode = \"bilinear\"\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "        self.conv = SignleConv(in_ch, out_ch)\n",
    "        self.recovery_CA = Recovery_CoordAtt(in_ch,in_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: [b,c, h, w]\n",
    "            x2: [b,c, 2*h,2*w]\n",
    "\n",
    "        Returns: 2x upsampled double conv reselt\n",
    "        \"\"\"\n",
    "        x = self.up(x1)\n",
    "\n",
    "        if x2 is not None:\n",
    "            x = self.recovery_CA(x2,x)\n",
    "            # x = torch.cat([x2, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Up4(nn.Module):\n",
    "    \"\"\"\n",
    "    Doubles spatial size with bilinear upsampling\n",
    "    Skip connections and double convs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Up4, self).__init__()\n",
    "        mode = \"bilinear\"\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "        self.conv = SignleConv(in_ch, out_ch)\n",
    "        self.recovery_CA = Recovery_CoordAtt(in_ch, in_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1: [b,c, h, w]\n",
    "            x2: [b,c, 2*h,2*w]\n",
    "\n",
    "        Returns: 2x upsampled double conv reselt\n",
    "        \"\"\"\n",
    "        x = self.up(x1)\n",
    "        if x2 is not None:\n",
    "            x = self.recovery_CA(x2, x)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "        x = x.view(B, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchExpand(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2 * dim, bias=False) if dim_scale == 2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        # print(\"x.size:\", x.size())\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C // 4)\n",
    "        # print(\"x.size:\", x.size())\n",
    "        x = x.view(B, -1, C // 4)\n",
    "        # print(\"x.size:\", x.size())\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalPatchExpand_X4(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, 16 * dim, bias=False)\n",
    "        self.output_dim = dim\n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale,\n",
    "                      c=C // (self.dim_scale ** 2))\n",
    "        x = x.view(B, -1, self.output_dim)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer_up(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, upsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if upsample is not None:\n",
    "            self.upsample = PatchExpand(input_resolution, dim=dim, dim_scale=2, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.upsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.upsample is not None:\n",
    "            x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerSys(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 2, 2], depths_decoder=[1, 2, 2, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, final_upsample=\"expand_first\", **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\n",
    "            \"SwinTransformerSys expand initial----depths:{};depths_decoder:{};drop_path_rate:{};num_classes:{}\".format(\n",
    "                depths,\n",
    "                depths_decoder, drop_path_rate, num_classes))\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.num_features_up = int(embed_dim * 2)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.final_upsample = final_upsample\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build encoder and bottleneck layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # build decoder layers\n",
    "        self.layers_up = nn.ModuleList()\n",
    "        self.concat_back_dim = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            concat_linear = nn.Linear(2 * int(embed_dim * 2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                      int(embed_dim * 2 ** (\n",
    "                                                  self.num_layers - 1 - i_layer))) if i_layer > 0 else nn.Identity()\n",
    "            if i_layer == 0:\n",
    "                layer_up = PatchExpand(\n",
    "                    input_resolution=(patches_resolution[0] // (2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                      patches_resolution[1] // (2 ** (self.num_layers - 1 - i_layer))),\n",
    "                    dim=int(embed_dim * 2 ** (self.num_layers - 1 - i_layer)), dim_scale=2, norm_layer=norm_layer)\n",
    "            else:\n",
    "                layer_up = BasicLayer_up(dim=int(embed_dim * 2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                         input_resolution=(\n",
    "                                         patches_resolution[0] // (2 ** (self.num_layers - 1 - i_layer)),\n",
    "                                         patches_resolution[1] // (2 ** (self.num_layers - 1 - i_layer))),\n",
    "                                         depth=depths[(self.num_layers - 1 - i_layer)],\n",
    "                                         num_heads=num_heads[(self.num_layers - 1 - i_layer)],\n",
    "                                         window_size=window_size,\n",
    "                                         mlp_ratio=self.mlp_ratio,\n",
    "                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                         drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                         drop_path=dpr[sum(depths[:(self.num_layers - 1 - i_layer)]):sum(\n",
    "                                             depths[:(self.num_layers - 1 - i_layer) + 1])],\n",
    "                                         norm_layer=norm_layer,\n",
    "                                         upsample=PatchExpand if (i_layer < self.num_layers - 1) else None,\n",
    "                                         use_checkpoint=use_checkpoint)\n",
    "            self.layers_up.append(layer_up)\n",
    "            self.concat_back_dim.append(concat_linear)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.norm_up = norm_layer(self.embed_dim)\n",
    "\n",
    "        self.dec1 = Up1(1152, 384)\n",
    "        self.dec2 = Up2(576, 192)\n",
    "        self.dec3 = Up3(288, 96)\n",
    "        self.dec4 = Up4(144, 48)\n",
    "        self.dec5 = FinalUp(48,24)\n",
    "        self.output1 = nn.Conv2d(in_channels=24, out_channels=self.num_classes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.CA_block_1 = CoordAtt(384, 384)\n",
    "        self.CA_block_2 = CoordAtt(192, 192)\n",
    "        self.CA_block_3 = CoordAtt(96, 96)\n",
    "\n",
    "        self.first_conv = DoubleConv(3,48)\n",
    "        self.first_pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # self.skip_CA = Skip_CoordAtt(384,384,192,192,96,96)\n",
    "\n",
    "        if self.final_upsample == \"expand_first\":\n",
    "            print(\"---final upsample expand_first---\")\n",
    "            self.up = FinalPatchExpand_X4(input_resolution=(img_size // patch_size, img_size // patch_size),\n",
    "                                          dim_scale=4, dim=embed_dim)\n",
    "            self.output = nn.Conv2d(in_channels=embed_dim, out_channels=self.num_classes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    # Encoder and Bottleneck\n",
    "    def forward_features(self, x):\n",
    "        first_skip_x = self.first_conv(x)\n",
    "        first_skip_x = self.first_pool(first_skip_x)\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        x_downsample = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_downsample.append(x)\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        return x, x_downsample, first_skip_x\n",
    "\n",
    "    # Dencoder and Skip connection\n",
    "    def forward_up_features(self, x, x_downsample):\n",
    "        for inx, layer_up in enumerate(self.layers_up):\n",
    "            if inx == 0:\n",
    "                x = layer_up(x)\n",
    "            else:\n",
    "                x = torch.cat([x, x_downsample[3 - inx]], -1)\n",
    "                x = self.concat_back_dim[inx](x)\n",
    "                x = layer_up(x)\n",
    "\n",
    "        x = self.norm_up(x)  # B L C\n",
    "\n",
    "        return x\n",
    "\n",
    "    def skip_reshape(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        H = int(L ** 0.5)\n",
    "        W = H\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, C, H, W)\n",
    "        # x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C // 4)\n",
    "        # x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def up_x4(self, x):\n",
    "        H, W = self.patches_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input features has wrong size\"\n",
    "\n",
    "        if self.final_upsample == \"expand_first\":\n",
    "            x = self.up(x)\n",
    "            x = x.view(B, 4 * H, 4 * W, -1)\n",
    "            x = x.permute(0, 3, 1, 2)  # B,C,H,W\n",
    "            x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x_downsample, first_skip_x = self.forward_features(x)\n",
    "        x = self.skip_reshape(x)\n",
    "        x_downsample[2] = self.skip_reshape(x_downsample[2])\n",
    "        x = self.dec1(x, x_downsample[2])\n",
    "        x_downsample[1] = self.skip_reshape(x_downsample[1])\n",
    "        x = self.dec2(x, x_downsample[1])\n",
    "        x_downsample[0] = self.skip_reshape(x_downsample[0])\n",
    "        x = self.dec3(x, x_downsample[0])\n",
    "        x = self.dec4(x, first_skip_x)\n",
    "        x = self.dec5(x)\n",
    "        x = self.output1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82645d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:02:15.880035Z",
     "iopub.status.busy": "2022-12-01T13:02:15.879437Z",
     "iopub.status.idle": "2022-12-01T13:02:16.160078Z",
     "shell.execute_reply": "2022-12-01T13:02:16.159062Z",
     "shell.execute_reply.started": "2022-12-01T13:02:15.879973Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "# from swin_transformer_unet_skip_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "# from swin_transformer_unet_skip_without_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "# from swin_transformer_unet_skip_100_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "# from swin_transformer_unet_skip_010_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "# from swin_transformer_unet_skip_001_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "# from swin_transformer_unet_skip_110_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "# from swin_transformer_unet_skip_011_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "# from swin_transformer_unet_skip_101_CA_recovery_expand_unetdecoder_sys import SwinTransformerSys\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SwinCAR(nn.Module):\n",
    "    def __init__(self, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
    "        super(SwinCAR, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        # self.config = config\n",
    "\n",
    "        self.swin_unet = SwinTransformerSys(img_size=224,\n",
    "                                            patch_size=4,\n",
    "                                            in_chans=3,\n",
    "                                            num_classes=self.num_classes,\n",
    "                                            embed_dim=96,\n",
    "                                            depths=[2, 2, 2, 2],\n",
    "                                            num_heads=[3, 6, 12, 24],\n",
    "                                            window_size=7,\n",
    "                                            mlp_ratio=4,\n",
    "                                            qkv_bias=True,\n",
    "                                            qk_scale=None,\n",
    "                                            drop_rate=0.0,\n",
    "                                            drop_path_rate=0.2,\n",
    "                                            ape=False,\n",
    "                                            patch_norm=True,\n",
    "                                            use_checkpoint=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        logits = self.swin_unet(x)\n",
    "        return logits\n",
    "\n",
    "    def load_from(self, pretrain_path):\n",
    "        pretrained_path = pretrain_path\n",
    "        if pretrained_path is not None:\n",
    "            print(\"pretrained_path:{}\".format(pretrained_path))\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            pretrained_dict = torch.load(pretrained_path, map_location=device)\n",
    "            if \"model\" not in pretrained_dict:\n",
    "                print(\"---start load pretrained modle by splitting---\")\n",
    "                pretrained_dict = {k[17:]: v for k, v in pretrained_dict.items()}\n",
    "                for k in list(pretrained_dict.keys()):\n",
    "                    if \"output\" in k:\n",
    "                        print(\"delete key:{}\".format(k))\n",
    "                        del pretrained_dict[k]\n",
    "                msg = self.swin_unet.load_state_dict(pretrained_dict, strict=False)\n",
    "                # print(msg)\n",
    "                return\n",
    "            pretrained_dict = pretrained_dict['model']\n",
    "            print(\"---start load pretrained modle of swin encoder---\")\n",
    "\n",
    "            model_dict = self.swin_unet.state_dict()\n",
    "            full_dict = copy.deepcopy(pretrained_dict)\n",
    "            for k, v in pretrained_dict.items():\n",
    "                if \"layers.\" in k:\n",
    "                    current_layer_num = 3 - int(k[7:8])\n",
    "                    current_k = \"layers_up.\" + str(current_layer_num) + k[8:]\n",
    "                    full_dict.update({current_k: v})\n",
    "            for k in list(full_dict.keys()):\n",
    "                if k in model_dict:\n",
    "                    if full_dict[k].shape != model_dict[k].shape:\n",
    "                        print(\"delete:{};shape pretrain:{};shape model:{}\".format(k, v.shape, model_dict[k].shape))\n",
    "                        del full_dict[k]\n",
    "\n",
    "            msg = self.swin_unet.load_state_dict(full_dict, strict=False)\n",
    "            print(msg)\n",
    "            print(\"--finish load pretrained model---\")\n",
    "        else:\n",
    "            print(\"none pretrain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085dde22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:02:18.845083Z",
     "iopub.status.busy": "2022-12-01T13:02:18.844500Z",
     "iopub.status.idle": "2022-12-01T13:02:19.049692Z",
     "shell.execute_reply": "2022-12-01T13:02:19.048920Z",
     "shell.execute_reply.started": "2022-12-01T13:02:18.845023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  \n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def CE_Loss(inputs, target, num_classes=21):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt = target.size()\n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "    temp_target = target.view(-1)\n",
    "\n",
    "    CE_loss  = nn.NLLLoss(ignore_index=num_classes)(F.log_softmax(temp_inputs, dim = -1), temp_target)\n",
    "    return CE_loss\n",
    "\n",
    "def Dice_loss(inputs, target, beta=1, smooth = 1e-5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    \n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    #--------------------------------------------#\n",
    "    #   Compute dice loss\n",
    "    #--------------------------------------------#\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    dice_loss = 1 - torch.mean(score)\n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587b43e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:02:22.254117Z",
     "iopub.status.busy": "2022-12-01T13:02:22.253540Z",
     "iopub.status.idle": "2022-12-01T13:02:22.284356Z",
     "shell.execute_reply": "2022-12-01T13:02:22.283306Z",
     "shell.execute_reply.started": "2022-12-01T13:02:22.254055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  \n",
    "\n",
    "\n",
    "def f_score(inputs, target, beta=1, smooth = 1e-5, threhold = 0.5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    \n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    pr = F.softmax(inputs[0].permute(1, 2, 0), dim=-1).cpu().numpy().argmax(axis=-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "    #--------------------------------------------#\n",
    "    #   Compute Dice\n",
    "    #--------------------------------------------#\n",
    "    temp_inputs = torch.gt(temp_inputs,threhold).float()\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    background = score[0]\n",
    "    pet = score[1]\n",
    "    score = torch.mean(score)\n",
    "    return score,background,pet,tp,fp,fn\n",
    "\n",
    "\n",
    "\n",
    "def Sensitivity(inputs, target, beta=1, smooth = 1e-5, threhold = 0.5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    \n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    temp_inputs = torch.gt(temp_inputs,threhold).float()\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = (tp + smooth) / ((beta ** 2) * tp + beta ** 2 * fn + smooth)\n",
    "    score = torch.mean(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def Precision(inputs, target, beta=1, smooth = 1e-5, threhold = 0.5):\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    \n",
    "    if h != ht and w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "    temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c),-1)\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "    #--------------------------------------------#\n",
    "    #   计算dice系数\n",
    "    #--------------------------------------------#\n",
    "    temp_inputs = torch.gt(temp_inputs,threhold).float()\n",
    "    tp = torch.sum(temp_target[...,:-1] * temp_inputs, axis=[0,1])\n",
    "    fp = torch.sum(temp_inputs                       , axis=[0,1]) - tp\n",
    "    fn = torch.sum(temp_target[...,:-1]              , axis=[0,1]) - tp\n",
    "\n",
    "    score = ( tp + smooth) / ( tp + fp + smooth)\n",
    "    score = torch.mean(score[1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c4d7d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:02:26.592243Z",
     "iopub.status.busy": "2022-12-01T13:02:26.591688Z",
     "iopub.status.idle": "2022-12-01T13:02:26.796306Z",
     "shell.execute_reply": "2022-12-01T13:02:26.795312Z",
     "shell.execute_reply.started": "2022-12-01T13:02:26.592185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def train_loss_plot(epoch,Model,BATCH_SIZE,dataset,loss):\n",
    "    num = epoch\n",
    "    Model = Model\n",
    "    batch_size = BATCH_SIZE\n",
    "    dataset = dataset\n",
    "    x = [i for i in range(num)]\n",
    "    plot_save_path = r'result/plot/'\n",
    "    if not os.path.exists(plot_save_path):\n",
    "        os.makedirs(plot_save_path)\n",
    "    save_loss = plot_save_path+str(Model)+'_'+str(batch_size)+'_'+str(dataset)+'_'+str(epoch)+'_train_loss.jpg'\n",
    "    plt.figure()\n",
    "    plt.plot(x,loss,label='loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_loss)\n",
    "    \n",
    "def val_loss_plot(epoch,Model,BATCH_SIZE,dataset,loss):\n",
    "    num = epoch\n",
    "    Model = Model\n",
    "    batch_size = BATCH_SIZE\n",
    "    dataset = dataset\n",
    "    x = [i for i in range(num)]\n",
    "    plot_save_path = r'result/plot/'\n",
    "    if not os.path.exists(plot_save_path):\n",
    "        os.makedirs(plot_save_path)\n",
    "    save_loss = plot_save_path+str(Model)+'_'+str(batch_size)+'_'+str(dataset)+'_'+str(epoch)+'_val_loss.jpg'\n",
    "    plt.figure()\n",
    "    plt.plot(x,loss,label='loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_loss)\n",
    "\n",
    "def metrics_plot(epoch,Model,BATCH_SIZE,dataset,name,*args):\n",
    "    num = epoch\n",
    "    Model = Model\n",
    "    batch_size = BATCH_SIZE\n",
    "    dataset = dataset\n",
    "    names = name.split('&')\n",
    "    metrics_value = args\n",
    "    i=0\n",
    "    x = [i for i in range(num)]\n",
    "    plot_save_path = r'result/plot/'\n",
    "    if not os.path.exists(plot_save_path):\n",
    "        os.makedirs(plot_save_path)\n",
    "    save_metrics = plot_save_path + str(Model) + '_' + str(batch_size) + '_' + str(dataset) + '_' + str(epoch) + '_'+name+'.jpg'\n",
    "    plt.figure()\n",
    "    for l in metrics_value:\n",
    "        plt.plot(x,l,label=str(names[i]))\n",
    "        #plt.scatter(x,l,label=str(l))\n",
    "        i+=1\n",
    "    plt.legend()\n",
    "    plt.savefig(save_metrics)\n",
    "    \n",
    "def test_metrics_plot(epoch,Model,BATCH_SIZE,dataset,name,*args):\n",
    "    num = epoch\n",
    "    Model = Model\n",
    "    batch_size = BATCH_SIZE\n",
    "    dataset = dataset\n",
    "    names = name.split('&')\n",
    "    metrics_value = args\n",
    "    i=0\n",
    "    # for i in range(num)\n",
    "\n",
    "    x = [i for i in range(num)]\n",
    "    plot_save_path = r'result/temp_out'\n",
    "    if not os.path.exists(plot_save_path):\n",
    "        os.makedirs(plot_save_path)\n",
    "    save_metrics = plot_save_path + str(Model) + '_' + str(batch_size) + '_' + str(dataset) + '_' + str(epoch) + '_'+name+'.jpg'\n",
    "    plt.figure()\n",
    "    print(metrics_value)\n",
    "    # for l in metrics_value:\n",
    "    plt.plot(x,metrics_value,label=str(names[i]))\n",
    "        #plt.scatter(x,l,label=str(l))\n",
    "        # i+=1\n",
    "    plt.legend()\n",
    "    plt.savefig(save_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a14b89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:02:31.027663Z",
     "iopub.status.busy": "2022-12-01T13:02:31.027088Z",
     "iopub.status.idle": "2022-12-01T13:02:31.040732Z",
     "shell.execute_reply": "2022-12-01T13:02:31.039316Z",
     "shell.execute_reply.started": "2022-12-01T13:02:31.027605Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "\n",
    "    fh = logging.FileHandler(filename, \"w\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61f5817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T11:57:29.466163Z",
     "iopub.status.busy": "2022-12-01T11:57:29.465732Z",
     "iopub.status.idle": "2022-12-01T11:57:29.878008Z",
     "shell.execute_reply": "2022-12-01T11:57:29.876917Z",
     "shell.execute_reply.started": "2022-12-01T11:57:29.466117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "# from plot import train_loss_plot, val_loss_plot, metrics_plot\n",
    "# from nets.unet_training import CE_Loss, Dice_loss\n",
    "# from utils.dataloader import DeeplabDataset, deeplab_dataset_collate\n",
    "# from utils.metrics import f_score\n",
    "# from nets.pspnet import PSPNet\n",
    "# from nets.deeplabv3_plus import DeepLab\n",
    "\n",
    "# from logger import get_logger\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def print_model_parm_nums(model):\n",
    "    total = sum([param.nelement() for param in model.parameters()])\n",
    "    print('  + Number of params: %.2fM' % (total / 1e6))\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def fit_one_epoch(net, epoch, epoch_size, epoch_size_val, gen, genval, Epoch, cuda):\n",
    "    total_loss = 0\n",
    "    total_f_score = 0\n",
    "\n",
    "    val_toal_loss = 0\n",
    "    val_total_f_score = 0\n",
    "\n",
    "    total_score = 0\n",
    "    total_background = 0\n",
    "    total_pet = 0\n",
    "    total_nonclass = 0\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    val_total_score = 0\n",
    "    val_total_background = 0\n",
    "    val_total_pet = 0\n",
    "    val_total_nonclass = 0\n",
    "    val_total_tp = 0\n",
    "    val_total_fn = 0\n",
    "    val_total_fp = 0\n",
    "\n",
    "    net = net.train()\n",
    "    with tqdm(total=epoch_size, desc=f'Epoch {epoch + 1}/{Epoch}', postfix=dict, mininterval=0.3) as pbar:\n",
    "        for iteration, batch in enumerate(gen):\n",
    "            if iteration >= epoch_size:\n",
    "                break\n",
    "            imgs, pngs, labels = batch\n",
    "\n",
    "            #             print(iteration.dtype())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # imgs = Variable(torch.from_numpy(imgs).type(torch.FloatTensor))\n",
    "                # pngs = Variable(torch.from_numpy(pngs).type(torch.FloatTensor)).long()\n",
    "                # labels = Variable(torch.from_numpy(labels).type(torch.FloatTensor))\n",
    "\n",
    "                imgs = Variable(imgs)\n",
    "                pngs = Variable(pngs).long()\n",
    "                labels = Variable(labels)\n",
    "                if cuda:\n",
    "                    imgs = imgs.cuda()\n",
    "                    pngs = pngs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "#             ttser = (torch.rand(1, 3, 224, 224),)\n",
    "#             flops = FlopCountAnalysis(net, ttser)\n",
    "#             print(\"FLOPs: \", flops.total())\n",
    "\n",
    "            outputs = net(imgs)\n",
    "            #             print(\"shape:\",outputs.shape)\n",
    "            loss = CE_Loss(outputs, pngs, num_classes=NUM_CLASSES)\n",
    "            #             print('CE_Loss:',loss)\n",
    "            if dice_loss:\n",
    "                main_dice = Dice_loss(outputs, labels)\n",
    "                #                 print('Dice_Loss:',main_dice)\n",
    "                loss = 0.5 * loss + 0.5 * main_dice\n",
    "            #                 loss      = main_dice\n",
    "            #                 loss      = loss + main_dice\n",
    "            #                 loss      = loss + main_dice\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # -------------------------------#\n",
    "                #   计算f_score\n",
    "                # -------------------------------#\n",
    "                #                 _f_score = f_score(outputs, labels)\n",
    "                score, background, pet, tp, fn, fp = f_score(outputs, labels)\n",
    "                _f_score = torch.mean(score)\n",
    "            #                 print(score)\n",
    "            #                 logger.info('iteration=={:.1f}\\t tp={:.8f}\\t fn={:.16f}\\t fp={:.8f}'.format((iteration+1), tp, fn, fp))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_score += score\n",
    "            total_background += background.item()\n",
    "            total_pet += pet.item()\n",
    "#             total_nonclass = nonclass.item()\n",
    "            total_tp += tp\n",
    "            total_fn += fn\n",
    "            total_fp += fp\n",
    "            total_loss += loss.item()\n",
    "            total_f_score += _f_score.item()\n",
    "            trian_avg_loss = total_loss / (iteration + 1)\n",
    "            pbar.set_postfix(**{'total_loss': total_loss / (iteration + 1),\n",
    "                                'train_pet': total_pet/(iteration+1),\n",
    "#                                 'train_nonclass': total_nonclass/(iteration+1),\n",
    "                                'f_score': total_f_score / (iteration + 1),\n",
    "                                'lr': get_lr(optimizer)})\n",
    "            pbar.update(1)\n",
    "\n",
    "        train_loss_list.append(trian_avg_loss)\n",
    "        logger.info('Epoch:[{}/{}]\\t train loss={:.8f}\\t train dice={:.8f}\\t lr={:.8f}\\n'.format(epoch + 1, Epoch,\n",
    "                                                                                                total_loss / (\n",
    "                                                                                                            iteration + 1),\n",
    "                                                                                                total_f_score / (\n",
    "                                                                                                            iteration + 1),\n",
    "                                                                                                get_lr(optimizer)))\n",
    "        logger.info('train score={}\\n'.format(total_score / (iteration + 1)))\n",
    "        logger.info(\n",
    "            'train background={}\\t pet={}\\t score={}\\n'.format(total_background / (iteration + 1),\n",
    "                                                                                        total_pet / (iteration + 1),\n",
    "                                                                                        total_f_score / (iteration + 1)))\n",
    "\n",
    "    net.eval()\n",
    "    print('Start Validation')\n",
    "    with tqdm(total=epoch_size_val, desc=f'Epoch {epoch + 1}/{Epoch}', postfix=dict, mininterval=0.3) as pbar:\n",
    "        for iteration, batch in enumerate(genval):\n",
    "            if iteration >= epoch_size_val:\n",
    "                break\n",
    "            imgs, pngs, labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                imgs = Variable(imgs)\n",
    "                pngs = Variable(pngs).long()\n",
    "                labels = Variable(labels)\n",
    "                if cuda:\n",
    "                    imgs = imgs.cuda()\n",
    "                    pngs = pngs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                outputs = net(imgs)\n",
    "                val_loss = CE_Loss(outputs, pngs, num_classes=NUM_CLASSES)\n",
    "                if dice_loss:\n",
    "                    main_dice = Dice_loss(outputs, labels)\n",
    "                    val_loss = 0.5 * val_loss + 0.5 * main_dice\n",
    "                #                     val_oss = main_dice\n",
    "                #                     val_loss = val_loss + main_dice\n",
    "\n",
    "                # -------------------------------#\n",
    "                #   计算f_score\n",
    "                # -------------------------------#\n",
    "                score, background, pet, tp, fn, fp = f_score(outputs, labels)\n",
    "                _f_score = torch.mean(score)\n",
    "                #                 print(score)\n",
    "                #                 print(_f_score)\n",
    "                #                 logger.info('iteration=={:.1f}\\t tp={:.8f}\\t fn={:.16f}\\t fp={:.8f}'.format((iteration+1), tp, fn, fp))\n",
    "\n",
    "                val_toal_loss += val_loss.item()\n",
    "                val_total_f_score += _f_score.item()\n",
    "                val_total_background += background.item()\n",
    "                val_total_pet += pet.item()\n",
    "#                 val_total_nonclass += nonclass.item()\n",
    "\n",
    "            val_total_score += score\n",
    "            val_total_tp += tp\n",
    "            val_total_fn += fn\n",
    "            val_total_fp += fp\n",
    "            Total_loss = val_toal_loss / (iteration + 1)\n",
    "            dice_score = val_total_f_score / (iteration + 1)\n",
    "            background_dice = val_total_background / (iteration+1)\n",
    "            pbar.set_postfix(**{'total_loss': val_toal_loss / (iteration + 1),\n",
    "                                'val_pet': val_total_pet / (iteration + 1),\n",
    "                                'f_score': val_total_f_score / (iteration + 1),\n",
    "                                'lr': get_lr(optimizer)})\n",
    "            pbar.update(1)\n",
    "\n",
    "        val_loss_list.append(Total_loss)\n",
    "        dice_list.append(dice_score)\n",
    "        logger.info(\n",
    "            'Epoch:[{}/{}]\\t iteration:{} val loss={:.8f}\\t val dice={:.8f}\\t lr={:.8f}'.format(epoch + 1, Epoch,\n",
    "                                                                                                 iteration + 1,\n",
    "                                                                                                 val_toal_loss / (\n",
    "                                                                                                             iteration + 1),\n",
    "                                                                                                 val_total_f_score / (\n",
    "                                                                                                             iteration + 1),\n",
    "                                                                                                 get_lr(optimizer)))\n",
    "        # logger.info(\n",
    "        #     'score={}\\t tp={}\\t fn={}\\t fp={}'.format(val_total_score / (iteration + 1), val_total_tp / (iteration + 1),\n",
    "        #                                               val_total_fn / (iteration + 1), val_total_fp / (iteration + 1)))\n",
    "        logger.info(\n",
    "            'val background={}\\t pet={}\\t score={}\\n'.format(val_total_background / (iteration+1), val_total_pet /(iteration+1), val_total_score / (iteration + 1)))\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    print('Finish Validation')\n",
    "    print('Epoch:' + str(epoch + 1) + '/' + str(Epoch))\n",
    "    print('Total Loss: %.4f' % (total_loss / (epoch_size + 1)))\n",
    "    print('Saving state, iter:', str(epoch + 1))\n",
    "    torch.save(model.state_dict(), 'logs/Epoch%d-train_loss%.4f-val_Loss%.4f-valbackgroundDice_%.4f-valDice_%.4f.pth' % (\n",
    "    (epoch + 1), train_loss_list[len(train_loss_list) - 1], Total_loss, background_dice, dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd92fb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T11:57:30.450446Z",
     "iopub.status.busy": "2022-12-01T11:57:30.449885Z",
     "iopub.status.idle": "2022-12-01T11:59:33.326301Z",
     "shell.execute_reply": "2022-12-01T11:59:33.324562Z",
     "shell.execute_reply.started": "2022-12-01T11:57:30.450390Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:2\n",
      "---final upsample expand_first---\n",
      "pretrained_path:pretrain/swin_tiny_patch4_window7_224.pth\n",
      "---start load pretrained modle of swin encoder---\n",
      "_IncompatibleKeys(missing_keys=['layers_up.0.expand.weight', 'layers_up.0.norm.weight', 'layers_up.0.norm.bias', 'layers_up.1.upsample.expand.weight', 'layers_up.1.upsample.norm.weight', 'layers_up.1.upsample.norm.bias', 'layers_up.2.upsample.expand.weight', 'layers_up.2.upsample.norm.weight', 'layers_up.2.upsample.norm.bias', 'concat_back_dim.1.weight', 'concat_back_dim.1.bias', 'concat_back_dim.2.weight', 'concat_back_dim.2.bias', 'concat_back_dim.3.weight', 'concat_back_dim.3.bias', 'norm_up.weight', 'norm_up.bias', 'dec1.conv.conv.0.conv.0.weight', 'dec1.conv.conv.0.conv.0.bias', 'dec1.conv.conv.0.conv.1.weight', 'dec1.conv.conv.0.conv.1.bias', 'dec1.conv.conv.0.conv.1.running_mean', 'dec1.conv.conv.0.conv.1.running_var', 'dec1.conv.conv.1.conv.0.weight', 'dec1.conv.conv.1.conv.0.bias', 'dec1.conv.conv.1.conv.1.weight', 'dec1.conv.conv.1.conv.1.bias', 'dec1.conv.conv.1.conv.1.running_mean', 'dec1.conv.conv.1.conv.1.running_var', 'dec1.recovery_CA.conv1.weight', 'dec1.recovery_CA.conv1.bias', 'dec1.recovery_CA.bn1.weight', 'dec1.recovery_CA.bn1.bias', 'dec1.recovery_CA.bn1.running_mean', 'dec1.recovery_CA.bn1.running_var', 'dec1.recovery_CA.conv.weight', 'dec1.recovery_CA.conv.bias', 'dec1.recovery_CA.conv1_h.weight', 'dec1.recovery_CA.conv1_h.bias', 'dec1.recovery_CA.conv1_w.weight', 'dec1.recovery_CA.conv1_w.bias', 'dec1.recovery_CA.acon.conv_frelu.weight', 'dec1.recovery_CA.acon.conv_frelu.bias', 'dec1.recovery_CA.acon.bn1.weight', 'dec1.recovery_CA.acon.bn1.bias', 'dec1.recovery_CA.acon.bn1.running_mean', 'dec1.recovery_CA.acon.bn1.running_var', 'dec1.recovery_CA.acon.bn2.weight', 'dec1.recovery_CA.acon.bn2.bias', 'dec1.recovery_CA.acon.bn2.running_mean', 'dec1.recovery_CA.acon.bn2.running_var', 'dec2.conv.conv.0.weight', 'dec2.conv.conv.0.bias', 'dec2.conv.conv.1.weight', 'dec2.conv.conv.1.bias', 'dec2.conv.conv.1.running_mean', 'dec2.conv.conv.1.running_var', 'dec2.recovery_CA.conv1.weight', 'dec2.recovery_CA.conv1.bias', 'dec2.recovery_CA.bn1.weight', 'dec2.recovery_CA.bn1.bias', 'dec2.recovery_CA.bn1.running_mean', 'dec2.recovery_CA.bn1.running_var', 'dec2.recovery_CA.conv.weight', 'dec2.recovery_CA.conv.bias', 'dec2.recovery_CA.conv1_h.weight', 'dec2.recovery_CA.conv1_h.bias', 'dec2.recovery_CA.conv1_w.weight', 'dec2.recovery_CA.conv1_w.bias', 'dec2.recovery_CA.acon.conv_frelu.weight', 'dec2.recovery_CA.acon.conv_frelu.bias', 'dec2.recovery_CA.acon.bn1.weight', 'dec2.recovery_CA.acon.bn1.bias', 'dec2.recovery_CA.acon.bn1.running_mean', 'dec2.recovery_CA.acon.bn1.running_var', 'dec2.recovery_CA.acon.bn2.weight', 'dec2.recovery_CA.acon.bn2.bias', 'dec2.recovery_CA.acon.bn2.running_mean', 'dec2.recovery_CA.acon.bn2.running_var', 'dec3.conv.conv.0.weight', 'dec3.conv.conv.0.bias', 'dec3.conv.conv.1.weight', 'dec3.conv.conv.1.bias', 'dec3.conv.conv.1.running_mean', 'dec3.conv.conv.1.running_var', 'dec3.recovery_CA.conv1.weight', 'dec3.recovery_CA.conv1.bias', 'dec3.recovery_CA.bn1.weight', 'dec3.recovery_CA.bn1.bias', 'dec3.recovery_CA.bn1.running_mean', 'dec3.recovery_CA.bn1.running_var', 'dec3.recovery_CA.conv.weight', 'dec3.recovery_CA.conv.bias', 'dec3.recovery_CA.conv1_h.weight', 'dec3.recovery_CA.conv1_h.bias', 'dec3.recovery_CA.conv1_w.weight', 'dec3.recovery_CA.conv1_w.bias', 'dec3.recovery_CA.acon.conv_frelu.weight', 'dec3.recovery_CA.acon.conv_frelu.bias', 'dec3.recovery_CA.acon.bn1.weight', 'dec3.recovery_CA.acon.bn1.bias', 'dec3.recovery_CA.acon.bn1.running_mean', 'dec3.recovery_CA.acon.bn1.running_var', 'dec3.recovery_CA.acon.bn2.weight', 'dec3.recovery_CA.acon.bn2.bias', 'dec3.recovery_CA.acon.bn2.running_mean', 'dec3.recovery_CA.acon.bn2.running_var', 'dec4.conv.conv.0.weight', 'dec4.conv.conv.0.bias', 'dec4.conv.conv.1.weight', 'dec4.conv.conv.1.bias', 'dec4.conv.conv.1.running_mean', 'dec4.conv.conv.1.running_var', 'dec4.recovery_CA.conv1.weight', 'dec4.recovery_CA.conv1.bias', 'dec4.recovery_CA.bn1.weight', 'dec4.recovery_CA.bn1.bias', 'dec4.recovery_CA.bn1.running_mean', 'dec4.recovery_CA.bn1.running_var', 'dec4.recovery_CA.conv.weight', 'dec4.recovery_CA.conv.bias', 'dec4.recovery_CA.conv1_h.weight', 'dec4.recovery_CA.conv1_h.bias', 'dec4.recovery_CA.conv1_w.weight', 'dec4.recovery_CA.conv1_w.bias', 'dec4.recovery_CA.acon.conv_frelu.weight', 'dec4.recovery_CA.acon.conv_frelu.bias', 'dec4.recovery_CA.acon.bn1.weight', 'dec4.recovery_CA.acon.bn1.bias', 'dec4.recovery_CA.acon.bn1.running_mean', 'dec4.recovery_CA.acon.bn1.running_var', 'dec4.recovery_CA.acon.bn2.weight', 'dec4.recovery_CA.acon.bn2.bias', 'dec4.recovery_CA.acon.bn2.running_mean', 'dec4.recovery_CA.acon.bn2.running_var', 'dec5.conv.conv.0.conv.0.weight', 'dec5.conv.conv.0.conv.0.bias', 'dec5.conv.conv.0.conv.1.weight', 'dec5.conv.conv.0.conv.1.bias', 'dec5.conv.conv.0.conv.1.running_mean', 'dec5.conv.conv.0.conv.1.running_var', 'dec5.conv.conv.1.conv.0.weight', 'dec5.conv.conv.1.conv.0.bias', 'dec5.conv.conv.1.conv.1.weight', 'dec5.conv.conv.1.conv.1.bias', 'dec5.conv.conv.1.conv.1.running_mean', 'dec5.conv.conv.1.conv.1.running_var', 'output1.weight', 'CA_block_1.conv1.weight', 'CA_block_1.conv1.bias', 'CA_block_1.bn1.weight', 'CA_block_1.bn1.bias', 'CA_block_1.bn1.running_mean', 'CA_block_1.bn1.running_var', 'CA_block_1.conv_h.weight', 'CA_block_1.conv_h.bias', 'CA_block_1.conv_w.weight', 'CA_block_1.conv_w.bias', 'CA_block_2.conv1.weight', 'CA_block_2.conv1.bias', 'CA_block_2.bn1.weight', 'CA_block_2.bn1.bias', 'CA_block_2.bn1.running_mean', 'CA_block_2.bn1.running_var', 'CA_block_2.conv_h.weight', 'CA_block_2.conv_h.bias', 'CA_block_2.conv_w.weight', 'CA_block_2.conv_w.bias', 'CA_block_3.conv1.weight', 'CA_block_3.conv1.bias', 'CA_block_3.bn1.weight', 'CA_block_3.bn1.bias', 'CA_block_3.bn1.running_mean', 'CA_block_3.bn1.running_var', 'CA_block_3.conv_h.weight', 'CA_block_3.conv_h.bias', 'CA_block_3.conv_w.weight', 'CA_block_3.conv_w.bias', 'first_conv.conv.0.conv.0.weight', 'first_conv.conv.0.conv.0.bias', 'first_conv.conv.0.conv.1.weight', 'first_conv.conv.0.conv.1.bias', 'first_conv.conv.0.conv.1.running_mean', 'first_conv.conv.0.conv.1.running_var', 'first_conv.conv.1.conv.0.weight', 'first_conv.conv.1.conv.0.bias', 'first_conv.conv.1.conv.1.weight', 'first_conv.conv.1.conv.1.bias', 'first_conv.conv.1.conv.1.running_mean', 'first_conv.conv.1.conv.1.running_var', 'up.expand.weight', 'up.norm.weight', 'up.norm.bias', 'output.weight'], unexpected_keys=['head.weight', 'head.bias', 'layers.2.blocks.2.norm1.weight', 'layers.2.blocks.2.norm1.bias', 'layers.2.blocks.2.attn.qkv.weight', 'layers.2.blocks.2.attn.qkv.bias', 'layers.2.blocks.2.attn.proj.weight', 'layers.2.blocks.2.attn.proj.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.weight', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.weight', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.norm1.weight', 'layers.2.blocks.3.norm1.bias', 'layers.2.blocks.3.attn.qkv.weight', 'layers.2.blocks.3.attn.qkv.bias', 'layers.2.blocks.3.attn.proj.weight', 'layers.2.blocks.3.attn.proj.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.weight', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.weight', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm1.weight', 'layers.2.blocks.4.norm1.bias', 'layers.2.blocks.4.attn.qkv.weight', 'layers.2.blocks.4.attn.qkv.bias', 'layers.2.blocks.4.attn.proj.weight', 'layers.2.blocks.4.attn.proj.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.weight', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.weight', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.blocks.5.norm1.weight', 'layers.2.blocks.5.norm1.bias', 'layers.2.blocks.5.attn.qkv.weight', 'layers.2.blocks.5.attn.qkv.bias', 'layers.2.blocks.5.attn.proj.weight', 'layers.2.blocks.5.attn.proj.bias', 'layers.2.blocks.5.norm2.weight', 'layers.2.blocks.5.norm2.bias', 'layers.2.blocks.5.mlp.fc1.weight', 'layers.2.blocks.5.mlp.fc1.bias', 'layers.2.blocks.5.mlp.fc2.weight', 'layers.2.blocks.5.mlp.fc2.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.2.attn.relative_position_bias_table', 'layers.2.blocks.3.attn.relative_position_bias_table', 'layers.2.blocks.4.attn.relative_position_bias_table', 'layers.2.blocks.5.attn.relative_position_bias_table', 'layers_up.0.blocks.0.norm1.weight', 'layers_up.0.blocks.0.norm1.bias', 'layers_up.0.blocks.0.attn.qkv.weight', 'layers_up.0.blocks.0.attn.qkv.bias', 'layers_up.0.blocks.0.attn.proj.weight', 'layers_up.0.blocks.0.attn.proj.bias', 'layers_up.0.blocks.0.norm2.weight', 'layers_up.0.blocks.0.norm2.bias', 'layers_up.0.blocks.0.mlp.fc1.weight', 'layers_up.0.blocks.0.mlp.fc1.bias', 'layers_up.0.blocks.0.mlp.fc2.weight', 'layers_up.0.blocks.0.mlp.fc2.bias', 'layers_up.0.blocks.1.norm1.weight', 'layers_up.0.blocks.1.norm1.bias', 'layers_up.0.blocks.1.attn.qkv.weight', 'layers_up.0.blocks.1.attn.qkv.bias', 'layers_up.0.blocks.1.attn.proj.weight', 'layers_up.0.blocks.1.attn.proj.bias', 'layers_up.0.blocks.1.norm2.weight', 'layers_up.0.blocks.1.norm2.bias', 'layers_up.0.blocks.1.mlp.fc1.weight', 'layers_up.0.blocks.1.mlp.fc1.bias', 'layers_up.0.blocks.1.mlp.fc2.weight', 'layers_up.0.blocks.1.mlp.fc2.bias', 'layers_up.0.blocks.0.attn.relative_position_index', 'layers_up.0.blocks.1.attn.relative_position_index', 'layers_up.0.blocks.0.attn.relative_position_bias_table', 'layers_up.0.blocks.1.attn.relative_position_bias_table', 'layers_up.1.downsample.norm.weight', 'layers_up.1.downsample.norm.bias', 'layers_up.1.downsample.reduction.weight', 'layers_up.1.blocks.2.norm1.weight', 'layers_up.1.blocks.2.norm1.bias', 'layers_up.1.blocks.2.attn.qkv.weight', 'layers_up.1.blocks.2.attn.qkv.bias', 'layers_up.1.blocks.2.attn.proj.weight', 'layers_up.1.blocks.2.attn.proj.bias', 'layers_up.1.blocks.2.norm2.weight', 'layers_up.1.blocks.2.norm2.bias', 'layers_up.1.blocks.2.mlp.fc1.weight', 'layers_up.1.blocks.2.mlp.fc1.bias', 'layers_up.1.blocks.2.mlp.fc2.weight', 'layers_up.1.blocks.2.mlp.fc2.bias', 'layers_up.1.blocks.3.norm1.weight', 'layers_up.1.blocks.3.norm1.bias', 'layers_up.1.blocks.3.attn.qkv.weight', 'layers_up.1.blocks.3.attn.qkv.bias', 'layers_up.1.blocks.3.attn.proj.weight', 'layers_up.1.blocks.3.attn.proj.bias', 'layers_up.1.blocks.3.norm2.weight', 'layers_up.1.blocks.3.norm2.bias', 'layers_up.1.blocks.3.mlp.fc1.weight', 'layers_up.1.blocks.3.mlp.fc1.bias', 'layers_up.1.blocks.3.mlp.fc2.weight', 'layers_up.1.blocks.3.mlp.fc2.bias', 'layers_up.1.blocks.4.norm1.weight', 'layers_up.1.blocks.4.norm1.bias', 'layers_up.1.blocks.4.attn.qkv.weight', 'layers_up.1.blocks.4.attn.qkv.bias', 'layers_up.1.blocks.4.attn.proj.weight', 'layers_up.1.blocks.4.attn.proj.bias', 'layers_up.1.blocks.4.norm2.weight', 'layers_up.1.blocks.4.norm2.bias', 'layers_up.1.blocks.4.mlp.fc1.weight', 'layers_up.1.blocks.4.mlp.fc1.bias', 'layers_up.1.blocks.4.mlp.fc2.weight', 'layers_up.1.blocks.4.mlp.fc2.bias', 'layers_up.1.blocks.5.norm1.weight', 'layers_up.1.blocks.5.norm1.bias', 'layers_up.1.blocks.5.attn.qkv.weight', 'layers_up.1.blocks.5.attn.qkv.bias', 'layers_up.1.blocks.5.attn.proj.weight', 'layers_up.1.blocks.5.attn.proj.bias', 'layers_up.1.blocks.5.norm2.weight', 'layers_up.1.blocks.5.norm2.bias', 'layers_up.1.blocks.5.mlp.fc1.weight', 'layers_up.1.blocks.5.mlp.fc1.bias', 'layers_up.1.blocks.5.mlp.fc2.weight', 'layers_up.1.blocks.5.mlp.fc2.bias', 'layers_up.1.blocks.2.attn.relative_position_index', 'layers_up.1.blocks.3.attn.relative_position_index', 'layers_up.1.blocks.4.attn.relative_position_index', 'layers_up.1.blocks.5.attn.relative_position_index', 'layers_up.1.blocks.3.attn_mask', 'layers_up.1.blocks.5.attn_mask', 'layers_up.1.blocks.2.attn.relative_position_bias_table', 'layers_up.1.blocks.3.attn.relative_position_bias_table', 'layers_up.1.blocks.4.attn.relative_position_bias_table', 'layers_up.1.blocks.5.attn.relative_position_bias_table', 'layers_up.2.downsample.norm.weight', 'layers_up.2.downsample.norm.bias', 'layers_up.2.downsample.reduction.weight', 'layers_up.3.downsample.norm.weight', 'layers_up.3.downsample.norm.bias', 'layers_up.3.downsample.reduction.weight'])\n",
      "--finish load pretrained model---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-01 11:57:37,919][<ipython-input-9-317cb1e253b9>][line:106][INFO] =========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "SwinCAR                                                 --                        --\n",
      "├─SwinTransformerSys: 1                                 --                        --\n",
      "│    └─ModuleList: 2-1                                  --                        --\n",
      "│    └─ModuleList: 2-2                                  --                        --\n",
      "│    └─ModuleList: 2-3                                  --                        --\n",
      "├─SwinTransformerSys: 1-1                               [8, 2, 224, 224]          --\n",
      "│    └─DoubleConv: 2-4                                  [8, 48, 224, 224]         --\n",
      "│    │    └─Sequential: 3-1                             [8, 48, 224, 224]         22,320\n",
      "│    └─MaxPool2d: 2-5                                   [8, 48, 112, 112]         --\n",
      "│    └─PatchEmbed: 2-6                                  [8, 3136, 96]             --\n",
      "│    │    └─Conv2d: 3-2                                 [8, 96, 56, 56]           4,704\n",
      "│    │    └─LayerNorm: 3-3                              [8, 3136, 96]             192\n",
      "│    └─Dropout: 2-7                                     [8, 3136, 96]             --\n",
      "│    └─ModuleList: 2-1                                  --                        --\n",
      "│    │    └─BasicLayer: 3-4                             [8, 784, 192]             299,190\n",
      "│    │    └─BasicLayer: 3-5                             [8, 196, 384]             1,188,204\n",
      "│    │    └─BasicLayer: 3-6                             [8, 49, 768]              4,735,704\n",
      "│    │    └─BasicLayer: 3-7                             [8, 49, 768]              14,183,856\n",
      "│    └─LayerNorm: 2-8                                   [8, 49, 768]              1,536\n",
      "│    └─Up1: 2-9                                         [8, 384, 14, 14]          --\n",
      "│    │    └─Upsample: 3-8                               [8, 768, 14, 14]          --\n",
      "│    │    └─DoubleConv: 3-9                             [8, 384, 14, 14]          5,310,720\n",
      "│    └─Up2: 2-10                                        [8, 192, 28, 28]          --\n",
      "│    │    └─Upsample: 3-10                              [8, 384, 28, 28]          --\n",
      "│    │    └─Recovery_CoordAtt: 3-11                     [8, 576, 28, 28]          84,744\n",
      "│    │    └─SignleConv: 3-12                            [8, 192, 28, 28]          995,904\n",
      "│    └─Up3: 2-11                                        [8, 96, 56, 56]           --\n",
      "│    │    └─Upsample: 3-13                              [8, 192, 56, 56]          --\n",
      "│    │    └─Recovery_CoordAtt: 3-14                     [8, 288, 56, 56]          21,636\n",
      "│    │    └─SignleConv: 3-15                            [8, 96, 56, 56]           249,120\n",
      "│    └─Up4: 2-12                                        [8, 48, 112, 112]         --\n",
      "│    │    └─Upsample: 3-16                              [8, 96, 112, 112]         --\n",
      "│    │    └─Recovery_CoordAtt: 3-17                     [8, 144, 112, 112]        5,634\n",
      "│    │    └─SignleConv: 3-18                            [8, 48, 112, 112]         62,352\n",
      "│    └─FinalUp: 2-13                                    [8, 24, 224, 224]         --\n",
      "│    │    └─Upsample: 3-19                              [8, 48, 224, 224]         --\n",
      "│    │    └─DoubleConv: 3-20                            [8, 24, 224, 224]         15,696\n",
      "│    └─Conv2d: 2-14                                     [8, 2, 224, 224]          48\n",
      "=========================================================================================================\n",
      "Total params: 27,138,189\n",
      "Trainable params: 27,138,189\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 42.64\n",
      "=========================================================================================================\n",
      "Input size (MB): 4.82\n",
      "Forward/backward pass size (MB): 1978.68\n",
      "Params size (MB): 108.55\n",
      "Estimated Total Size (MB): 2092.05\n",
      "=========================================================================================================\n",
      "[2022-12-01 11:57:37,922][<ipython-input-9-317cb1e253b9>][line:107][INFO] start training!\n",
      "Epoch 1/200: 100%|█████████▉| 553/554 [01:28<00:00,  6.43it/s, f_score=0.862, lr=0.0001, total_loss=0.295, train_pet=0.789][2022-12-01 11:59:06,560][<ipython-input-8-81ce67798ec7>][line:132][INFO] Epoch:[1/200]\t train loss=0.29534050\t train dice=0.86166095\t lr=0.00010000\n",
      "\n",
      "[2022-12-01 11:59:06,562][<ipython-input-8-81ce67798ec7>][line:133][INFO] train score=0.8616605997085571\n",
      "\n",
      "[2022-12-01 11:59:06,563][<ipython-input-8-81ce67798ec7>][line:137][INFO] train background=0.9344106744773121\t pet=0.7889112256709419\t score=0.8616609500203322\n",
      "\n",
      "Epoch 1/200: 100%|██████████| 554/554 [01:28<00:00,  6.25it/s, f_score=0.862, lr=0.0001, total_loss=0.295, train_pet=0.789]\n",
      "Epoch 1/200:   0%|          | 0/184 [00:00<?, ?it/s<class 'dict'>]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:  99%|█████████▉| 183/184 [00:09<00:00, 19.95it/s, f_score=0.908, lr=0.0001, total_loss=0.197, val_pet=0.854][2022-12-01 11:59:16,471][<ipython-input-8-81ce67798ec7>][line:201][INFO] Epoch:[1/200]\t iteration:184 val loss=0.19743351\t val dice=0.90766570\t lr=0.00010000\n",
      "[2022-12-01 11:59:16,474][<ipython-input-8-81ce67798ec7>][line:206][INFO] val background=0.9609312891312267\t pet=0.8544001151686129\t score=0.9076657891273499\n",
      "\n",
      "Epoch 1/200: 100%|██████████| 184/184 [00:09<00:00, 18.58it/s, f_score=0.908, lr=0.0001, total_loss=0.197, val_pet=0.854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Validation\n",
      "Epoch:1/200\n",
      "Total Loss: 0.2948\n",
      "Saving state, iter: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200:  18%|█▊        | 99/554 [00:16<01:14,  6.11it/s, f_score=0.908, lr=9.98e-5, total_loss=0.202, train_pet=0.857]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-317cb1e253b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start training!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInit_Epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterval_Epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mtrainval_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_size_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterval_Epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-81ce67798ec7>\u001b[0m in \u001b[0;36mfit_one_epoch\u001b[0;34m(net, epoch, epoch_size, epoch_size_val, gen, genval, Epoch, cuda)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#             print(\"FLOPs: \", flops.total())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;31m#             print(\"shape:\",outputs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCE_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpngs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-17e14d5231f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswin_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3a3403e15e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_downsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_skip_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mx_downsample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_downsample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3a3403e15e8>\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;31m# Encoder and Bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mfirst_skip_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m         \u001b[0mfirst_skip_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_skip_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3a3403e15e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3a3403e15e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    717\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 self._forward_pre_hooks.values()):\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    log_dir = \"logs/\"\n",
    "    # ------------------------------#\n",
    "    #   The size of the input image\n",
    "    # ------------------------------#\n",
    "    inputs_size = [224, 224, 3]\n",
    "    # ---------------------#\n",
    "    #   The number of segmentation classes\n",
    "    # ---------------------#\n",
    "    NUM_CLASSES = 2\n",
    "    # --------------------------------------------------------------------#\n",
    "    #   Enable the dice loss\n",
    "    # ---------------------------------------------------------------------#\n",
    "    dice_loss = True\n",
    "    # -------------------------------#\n",
    "    #   Enable Cuda\n",
    "    # -------------------------------#\n",
    "    Cuda = True\n",
    "    # -------------------------------#\n",
    "    #   Set batch size. \n",
    "    #   And here the variable of dataset and model just is for the name of loss figure\n",
    "    # -------------------------------#\n",
    "    BATCH_SIZE = 8\n",
    "    dataset = 'oxford-iiit-pet'\n",
    "    Model = 'SwinCAR'\n",
    "\n",
    "    # -------------------------------#\n",
    "    #   Set the path of pretrain model of Swin Transformer\n",
    "    # -------------------------------#\n",
    "    pretrain_path = 'pretrain/swin_tiny_patch4_window7_224.pth'\n",
    "\n",
    "    \n",
    "    # -------------------------------#\n",
    "    #   Obtain the model\n",
    "    #   Replace the comment with whichever model you want\n",
    "    # -------------------------------#\n",
    "    \n",
    "#     Unet\n",
    "#     model = Unet(num_classes=NUM_CLASSES, in_channels=inputs_size[-1], pretrained=pretrained).train()\n",
    "\n",
    "#     SwinCAR\n",
    "    model = SwinCAR(img_size=224, num_classes=NUM_CLASSES)\n",
    "    model.load_from(pretrain_path)\n",
    "    \n",
    "#     PSPNet\n",
    "    # model = PSPNet(num_classes=NUM_CLASSES, backbone=\"resnet50\", downsample_factor=16, pretrained=False, aux_branch=False)\n",
    "\n",
    "    # DeepLabV3+\n",
    "#     model = DeepLab(num_classes=NUM_CLASSES, backbone=\"xception\", downsample_factor=16, pretrained=False)\n",
    "\n",
    "#     Attention UNet\n",
    "    # model = AttU_Net(3,NUM_CLASSES).train()\n",
    "\n",
    "\n",
    "    if Cuda:\n",
    "        net = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "        net = net.cuda()\n",
    "\n",
    "    # txt file of train set. This file records names of each image in the train set\n",
    "    with open(r\"Medical_Datasets/ImageSets/cv_project/train.txt\", \"r\") as f:\n",
    "        train_lines = f.readlines()\n",
    "    # txt file of val set. This file records names of each image in the val set\n",
    "    with open(r\"Medical_Datasets/ImageSets/cv_project/val.txt\", \"r\") as f:\n",
    "        val_lines = f.readlines()\n",
    "\n",
    "    log = pd.DataFrame(index=[], columns=[\n",
    "        'epoch', 'lr', 'train_loss', 'train_stroke_dice', 'train_pet_dice', 'train_dice', 'val_loss', 'val_stroke_dice', 'val_pet_dice', 'val_dice'\n",
    "    ])\n",
    "    stroke_dice_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    dice_list = []\n",
    "    # ------------------------------------------------------#\n",
    "    #   lr: learning rate\n",
    "    #   Init_Epoch: \n",
    "    #   Interval_Epoch: the number of training epochs\n",
    "    # ------------------------------------------------------#\n",
    "    if True:\n",
    "        lr = 1e-4\n",
    "        Init_Epoch = 0\n",
    "        Interval_Epoch = 200\n",
    "        Batch_size = 8\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr, weight_decay=0.01)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 32, eta_min=7e-6, last_epoch=-1)\n",
    "\n",
    "        train_dataset = DeeplabDataset(train_lines, inputs_size, NUM_CLASSES, False,\n",
    "                                       dataset_path = '/openbayes/input/input3')\n",
    "        val_dataset = DeeplabDataset(val_lines, inputs_size, NUM_CLASSES, False,\n",
    "                                     dataset_path = '/openbayes/input/input3')\n",
    "        gen = DataLoader(train_dataset, shuffle=True, batch_size=Batch_size, num_workers=4, pin_memory=True,\n",
    "                         drop_last=True, collate_fn=deeplab_dataset_collate)\n",
    "        gen_val = DataLoader(val_dataset, shuffle=False, batch_size=Batch_size, num_workers=4, pin_memory=True,\n",
    "                             drop_last=True, collate_fn=deeplab_dataset_collate)\n",
    "\n",
    "        epoch_size = max(1, len(train_lines) // Batch_size)\n",
    "        epoch_size_val = max(1, len(val_lines) // Batch_size)\n",
    "\n",
    "        logger = get_logger('save_log/train_log.log')\n",
    "        logger.info(summary(model, input_size=(Batch_size,3,224,224)))\n",
    "        logger.info('start training!')\n",
    "        for epoch in range(Init_Epoch, Interval_Epoch):\n",
    "            trainval_log = fit_one_epoch(model, epoch, epoch_size, epoch_size_val, gen, gen_val, Interval_Epoch, Cuda)\n",
    "            lr_scheduler.step()\n",
    "\n",
    "\n",
    "        logger.info('finish training!')\n",
    "        train_loss_plot(Interval_Epoch, Model, BATCH_SIZE, dataset, train_loss_list)\n",
    "        val_loss_plot(Interval_Epoch, Model, BATCH_SIZE, dataset, val_loss_list)\n",
    "        metrics_plot(Interval_Epoch, Model, BATCH_SIZE, dataset, 'trianloss&valloss', train_loss_list, val_loss_list)\n",
    "        metrics_plot(Interval_Epoch, Model, BATCH_SIZE, dataset, 'dice', dice_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5494aa73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:07:53.911196Z",
     "iopub.status.busy": "2022-12-01T13:07:53.910610Z",
     "iopub.status.idle": "2022-12-01T13:07:53.953036Z",
     "shell.execute_reply": "2022-12-01T13:07:53.951965Z",
     "shell.execute_reply.started": "2022-12-01T13:07:53.911135Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "import cv2\n",
    "\n",
    "def letterbox_image(image, label , size):\n",
    "    label = Image.fromarray(np.array(label))\n",
    "    '''resize image with unchanged aspect ratio using padding'''\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "    scale = min(w/iw, h/ih)\n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "\n",
    "    image = image.resize((nw,nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128,128,128))\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    label = label.resize((nw,nh), Image.NEAREST)\n",
    "    new_label = Image.new('L', size, (0))\n",
    "    new_label.paste(label, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    return new_image, new_label\n",
    "\n",
    "def rand(a=0, b=1):\n",
    "    return np.random.rand()*(b-a) + a\n",
    "\n",
    "class Test_DeeplabDataset(Dataset):\n",
    "    def __init__(self,train_lines,image_size,num_classes,random_data,dataset_path):\n",
    "        super(Test_DeeplabDataset, self).__init__()\n",
    "\n",
    "        self.train_lines = train_lines\n",
    "        self.train_batches = len(train_lines)\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.random_data = random_data\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_batches\n",
    "\n",
    "    def rand(self, a=0, b=1):\n",
    "        return np.random.rand() * (b - a) + a\n",
    "\n",
    "    def get_random_data(self, image, label, input_shape, jitter=.1, hue=.0, sat=1.1, val=1.1):\n",
    "        image = image.convert(\"RGB\")\n",
    "        label = Image.fromarray(np.array(label))\n",
    "\n",
    "        h, w = input_shape\n",
    "        # resize image\n",
    "        rand_jit1 = rand(1-jitter,1+jitter)\n",
    "        rand_jit2 = rand(1-jitter,1+jitter)\n",
    "        new_ar = w/h * rand_jit1/rand_jit2\n",
    "\n",
    "        scale = rand(0.5,1.5)\n",
    "        if new_ar < 1:\n",
    "            nh = int(scale*h)\n",
    "            nw = int(nh*new_ar)\n",
    "        else:\n",
    "            nw = int(scale*w)\n",
    "            nh = int(nw/new_ar)\n",
    "        image = image.resize((nw,nh), Image.BICUBIC)\n",
    "        label = label.resize((nw,nh), Image.NEAREST)\n",
    "        label = label.convert(\"L\")\n",
    "        \n",
    "        # flip image or not\n",
    "        flip = rand()<.5\n",
    "        if flip: \n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        # place image\n",
    "        dx = int(rand(0, w-nw))\n",
    "        dy = int(rand(0, h-nh))\n",
    "        new_image = Image.new('RGB', (w,h), (128,128,128))\n",
    "        new_label = Image.new('L', (w,h), (0))\n",
    "        new_image.paste(image, (dx, dy))\n",
    "        new_label.paste(label, (dx, dy))\n",
    "        image = new_image\n",
    "        label = new_label\n",
    "\n",
    "        # distort image\n",
    "        hue = rand(-hue, hue)\n",
    "        sat = rand(1, sat) if rand()<.5 else 1/rand(1, sat)\n",
    "        val = rand(1, val) if rand()<.5 else 1/rand(1, val)\n",
    "        x = cv2.cvtColor(np.array(image,np.float32)/255, cv2.COLOR_RGB2HSV)\n",
    "        x[..., 0] += hue*360\n",
    "        x[..., 0][x[..., 0]>1] -= 1\n",
    "        x[..., 0][x[..., 0]<0] += 1\n",
    "        x[..., 1] *= sat\n",
    "        x[..., 2] *= val\n",
    "        x[x[:,:, 0]>360, 0] = 360\n",
    "        x[:, :, 1:][x[:, :, 1:]>1] = 1\n",
    "        x[x<0] = 0\n",
    "        image_data = cv2.cvtColor(x, cv2.COLOR_HSV2RGB)*255\n",
    "        return image_data,label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # if index == 0:\n",
    "        #     shuffle(self.train_lines)\n",
    "            \n",
    "        annotation_line = self.train_lines[index]\n",
    "        name = annotation_line.split()[0]\n",
    "        \n",
    "        jpg_path = self.dataset_path + '/test_image/' + name + \".png\"\n",
    "        png_path = self.dataset_path + '/test_label/' + name + \".png\"\n",
    "\n",
    "        jpg = Image.open(os.path.join(os.path.join(self.dataset_path, \"test_image\"), name + \".png\"))\n",
    "        png = Image.open(os.path.join(os.path.join(self.dataset_path, \"test_label\"), name + \".png\"))\n",
    "\n",
    "        # 从文件中读取图像\n",
    "        png = np.array(png)\n",
    "        png[png >= self.num_classes] = self.num_classes\n",
    "        \n",
    "        seg_labels = np.eye(self.num_classes+1)[png.reshape([-1])]\n",
    "        seg_labels = seg_labels.reshape((int(self.image_size[1]),int(self.image_size[0]),self.num_classes+1))\n",
    "\n",
    "        jpg = np.transpose(np.array(jpg),[2,0,1])/255\n",
    "\n",
    "        return jpg, png, seg_labels,jpg_path ,png_path\n",
    "    \n",
    "    \n",
    "\n",
    "# DataLoader中collate_fn使用\n",
    "def Test_deeplab_dataset_collate(batch):\n",
    "    images = []\n",
    "    pngs = []\n",
    "    seg_labels = []\n",
    "    image_path = []\n",
    "    label_path = []\n",
    "    for img, png, labels,img_path,labels_path in batch:\n",
    "        images.append(img)\n",
    "        pngs.append(png)\n",
    "        seg_labels.append(labels)\n",
    "        image_path.append(img_path)\n",
    "        label_path.append(labels_path)\n",
    "    images = np.array(images)\n",
    "    pngs = np.array(pngs)\n",
    "    seg_labels = np.array(seg_labels)\n",
    "    return images, pngs, seg_labels,image_path,label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6b5357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T13:13:41.222588Z",
     "iopub.status.busy": "2022-12-01T13:13:41.222160Z",
     "iopub.status.idle": "2022-12-01T13:13:48.575345Z",
     "shell.execute_reply": "2022-12-01T13:13:48.573983Z",
     "shell.execute_reply.started": "2022-12-01T13:13:41.222544Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-01 13:13:41,305][<ipython-input-13-481c0336c26d>][line:214][INFO] start testing!\n",
      "[2022-12-01 13:13:41,305][<ipython-input-13-481c0336c26d>][line:214][INFO] start testing!\n",
      "[2022-12-01 13:13:41,305][<ipython-input-13-481c0336c26d>][line:214][INFO] start testing!\n",
      "[2022-12-01 13:13:41,305][<ipython-input-13-481c0336c26d>][line:214][INFO] start testing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:2\n",
      "---final upsample expand_first---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/1470 [00:00<?, ?it/s<class 'dict'>]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/Epoch1-train_loss0.2482-val_Loss0.1643-valStrokeDice_0.9627-valDice_0.9093.pth model loaded.\n",
      "Start Validation\n",
      "['/openbayes/input/input3/test_image/Abyssinian_111.png'] : tensor(1.0915e-09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/1470 [00:02<?, ?it/s<class 'dict'>]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/openbayes/input/input3/test_image/Abyssinian_111.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'hd' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-481c0336c26d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mepoch_size_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_lines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mfit_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mgen_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilesname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finish testing!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-481c0336c26d>\u001b[0m in \u001b[0;36mfit_one_epoch\u001b[0;34m(net, genval, cuda, filesname)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecificity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hd:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mval_total_hd\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mhd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mval_total_sc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'hd' referenced before assignment"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from medpy import metric\n",
    "# from nets.unet import Unet\n",
    "# from utils.metrics import f_score\n",
    "# from utils.metrics import Sensitivity\n",
    "# from utils.metrics import Precision\n",
    "# from nets.unet import Unet\n",
    "import matplotlib.pyplot as plt\n",
    "# from Attention_Unet_from_UnetZoo import AttU_Net\n",
    "# from plot import test_metrics_plot\n",
    "# from load_swin_transformer import SwinUnet as ViT_seg\n",
    "from logger import get_logger\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def fit_one_epoch(net, genval, cuda,filesname):\n",
    "\n",
    "    val_total_f_score = 0\n",
    "    val_total_sensitivity = 0\n",
    "    val_total_precision = 0\n",
    "    val_total_hd = 0\n",
    "    val_total_sc = 0\n",
    "    val_total_sp = 0\n",
    "    val_total_jc = 0\n",
    "    val_total_background = 0\n",
    "    val_total_pet = 0\n",
    "    val_total_nonclass = 0\n",
    "\n",
    "    i = 0\n",
    "    net.eval()\n",
    "    print('Start Validation')\n",
    "    with tqdm(total=1470, desc=f'Epoch {0 + 1}/{1}', postfix=dict, mininterval=0.3) as pbar:\n",
    "        for iteration, batch in enumerate(genval):\n",
    "            if iteration >= epoch_size_val:\n",
    "                break\n",
    "            imgs, pngs, labels,imgs_path,label_path = batch\n",
    "            image = np.squeeze(imgs, 0)\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            image = Image.fromarray(np.uint8(image))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                imgs = Variable(torch.from_numpy(imgs).type(torch.FloatTensor))\n",
    "                pngs = Variable(torch.from_numpy(pngs).type(torch.FloatTensor)).long()\n",
    "                labels = Variable(torch.from_numpy(labels).type(torch.FloatTensor))\n",
    "                if cuda:\n",
    "                    imgs = imgs.cuda()\n",
    "                    pngs = pngs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                # outputs = net(imgs).cuda()\n",
    "                outputs = net(imgs)\n",
    "                # -------------------------------#\n",
    "                #   计算f_score\n",
    "                # -------------------------------#\n",
    "                _f_score, background_score, pet,tp,fp,fn = f_score(outputs, labels)\n",
    "                sensitivity = Sensitivity(outputs,labels)\n",
    "                precision = Precision(outputs,labels)\n",
    "                i +=1\n",
    "                print(str(imgs_path),':',pet)\n",
    "\n",
    "                val_total_f_score += _f_score.item() if (_f_score != 0) else None\n",
    "                val_total_sensitivity += sensitivity.item() if (sensitivity != 0) else None\n",
    "                val_total_precision += precision.item() if (precision != 0) else None\n",
    "                predict = torch.squeeze(outputs,0).cpu().numpy()\n",
    "                predict = predict.transpose(1, 2, 0)\n",
    "                predict = Image.fromarray(np.uint8(predict))\n",
    "                pr = torch.squeeze(outputs,0)\n",
    "                pr = F.softmax(pr.permute(1, 2, 0), dim=-1).cpu().numpy().argmax(axis=-1)\n",
    "\n",
    "                val_total_background += background_score.item()\n",
    "                val_total_pet += pet.item()\n",
    "                background_dice_list.append(background_score)\n",
    "                pet_dice_list.append(pet)\n",
    "\n",
    "\n",
    "            seg_img = np.zeros((np.shape(pr)[0], np.shape(pr)[1], 3))\n",
    "\n",
    "            for c in range(num_classes):\n",
    "                seg_img[:, :, 0] += ((pr[:, :] == c) * (colors[c][0])).astype('uint8')\n",
    "                seg_img[:, :, 1] += ((pr[:, :] == c) * (colors[c][1])).astype('uint8')\n",
    "                seg_img[:, :, 2] += ((pr[:, :] == c) * (colors[c][2])).astype('uint8')\n",
    "            \n",
    "            r_image = Image.fromarray(np.uint8(seg_img))\n",
    "            r_image = r_image.convert('L')\n",
    "            r_image = np.array(r_image)\n",
    "            r_image[r_image == 0] = 1\n",
    "            r_image[r_image == 255] = 0\n",
    "\n",
    "            label = cv2.imread(label_path[0])\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_subplot(1, 3, 1)\n",
    "            ax1.set_title('input')\n",
    "            plt.imshow(Image.open(imgs_path[0]))\n",
    "            ax2 = fig.add_subplot(1, 3, 2)\n",
    "            ax2.set_title('predict')\n",
    "            print(imgs_path[0])\n",
    "            cv2.imwrite('result/plot/tempout1/' + imgs_path[0].split('/')[-1], r_image)\n",
    "            plt.imshow(r_image, cmap='Greys_r')\n",
    "            ax3 = fig.add_subplot(1, 3, 3)\n",
    "            ax3.set_title('mask')\n",
    "            plt.imshow(Image.open(label_path[0]), cmap='Greys_r')\n",
    "            mm = Image.open(label_path[0])\n",
    "            mm = np.array(mm)\n",
    "            if cv2.countNonZero(r_image) != 0:\n",
    "                hd = metric.hd(r_image,mm)\n",
    "                jc = metric.jc(r_image,mm)\n",
    "                sc = metric.sensitivity(r_image,mm)\n",
    "                sp = metric.specificity(r_image,mm)\n",
    "\n",
    "            print('hd:',hd)\n",
    "            val_total_hd += hd\n",
    "            val_total_sc += sc\n",
    "            val_total_sp += sp\n",
    "            val_total_jc += jc\n",
    "            print('jc:', jc)\n",
    "            print('sc:', sc)\n",
    "            print('sp:', sp)\n",
    "            plt.savefig('result/plot/tempout1/savefig_' + imgs_path[0].split('/')[-1], bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "            average_dice_score = val_total_f_score / (iteration+1)\n",
    "            average_sensitivity = val_total_sensitivity / (iteration+1)\n",
    "            average_precision = val_total_precision / (iteration+1)\n",
    "            average_background_dice = val_total_background / (iteration+1)\n",
    "            average_backgroun = val_total_pet / (iteration + 1)\n",
    "            average_hd = val_total_hd / (iteration+1)\n",
    "            average_sc = val_total_sc / (iteration+1)\n",
    "            average_sp = val_total_sp / (iteration + 1)\n",
    "            average_jc = val_total_jc / (iteration + 1)\n",
    "            pbar.set_postfix(**{'f_score': val_total_f_score / (iteration + 1)})\n",
    "            pbar.update(1)\n",
    "\n",
    "    dice_list.append(average_dice_score)\n",
    "    sensitivity_list.append(average_sensitivity)\n",
    "    precision_list.append(average_precision)\n",
    "    logger.info('filesname={}\\t average_dice={:.16f}\\t average_sensitivity={:.16f}\\t average_precision={:.16f}'.format(filesname, average_dice_score, average_sensitivity, average_precision ))\n",
    "    print(filesname,' average_dice:',average_dice_score)\n",
    "    print(filesname,' average_sensitivity:',average_sensitivity)\n",
    "    print(filesname,' average_precision:',average_precision)\n",
    "    print(filesname,' average_background_dice:',average_background_dice)\n",
    "    print(filesname,' average_backgroun:',average_backgroun)\n",
    "    print(filesname, ' average_hd:', average_hd)\n",
    "    print(filesname, ' average_sc:', average_sc)\n",
    "    print(filesname, ' average_sp:', average_sp)\n",
    "    print(filesname, ' average_jc:', average_jc)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log_dir = \"logs/\"\n",
    "    # ------------------------------#\n",
    "    #   输入图片的大小\n",
    "    # ------------------------------#\n",
    "    inputs_size = [224, 224, 3]\n",
    "    # ---------------------#\n",
    "    #   分类个数+1\n",
    "    #   背景+边缘\n",
    "    # ---------------------#\n",
    "    NUM_CLASSES = 2\n",
    "    # --------------------------------------------------------------------#\n",
    "    #   建议选项：\n",
    "    #   种类少（几类）时，设置为True\n",
    "    #   种类多（十几类）时，如果batch_size比较大（10以上），那么设置为True\n",
    "    #   种类多（十几类）时，如果batch_size比较小（10以下），那么设置为False\n",
    "    # ---------------------------------------------------------------------#\n",
    "    dice_loss = True\n",
    "    # -------------------------------#\n",
    "    #   主干网络预训练权重的使用\n",
    "    # -------------------------------#\n",
    "    pretrained = False\n",
    "    # -------------------------------#\n",
    "    #   Cuda的使用\n",
    "    # -------------------------------#\n",
    "    Cuda = False\n",
    "\n",
    "    # -------------------------------#\n",
    "    #   plot.py中图片命名设定\n",
    "    #   Epoch在最下面\n",
    "    # -------------------------------#\n",
    "    BATCH_SIZE = 1\n",
    "    dataset = 'oxford-iiit-pet'\n",
    "    Model = 'SwinCAR'\n",
    "\n",
    "\n",
    "    if Cuda:\n",
    "#         net = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "#         net = net.cuda()\n",
    "\n",
    "    # 打开数据集的txt\n",
    "    with open(\"Medical_Datasets/ImageSets/cv_project/test.txt\", \"r\") as f:\n",
    "        val_lines = f.readlines()\n",
    "\n",
    "    background_dice_list = []\n",
    "    pet_dice_list = []\n",
    "    nonclass_dice_list = []\n",
    "    loss_list = []\n",
    "    dice_list = []\n",
    "    sensitivity_list = []\n",
    "    precision_list = []\n",
    "    hd_list = []\n",
    "    logger = get_logger('save_log/test_log.log')\n",
    "    logger.info('start testing!')\n",
    "    for root, dirs,files in os.walk('logs'):\n",
    "        for filesname in sorted(files):\n",
    "            model_path = 'logs/'+filesname\n",
    "            model_image_size= [224, 224, 3]\n",
    "            num_classes = 2\n",
    "            cuda = False\n",
    "            # --------------------------------#\n",
    "            #   blend参数用于控制是否\n",
    "            #   让识别结果和原图混合\n",
    "            # --------------------------------#\n",
    "            blend= False\n",
    "\n",
    "\n",
    "            # ---------------------------------------------------#\n",
    "            #   获得所有的分类\n",
    "            # ---------------------------------------------------#\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#             net = Unet(num_classes=num_classes, in_channels=model_image_size[-1]).eval()\n",
    "#             net = TransUnet(in_channels=3, img_dim=224, vit_blocks=12, vit_heads=12 ,vit_dim_linear_mhsa_block=3072, classes=2).eval()\n",
    "#             net = SUnet(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32),num_classes=2).eval()\n",
    "            net = SwinCAR(img_size=224, num_classes=NUM_CLASSES).eval()\n",
    "#             net = Unet_from_UnetZoo(3,NUM_CLASSES).eval()\n",
    "#             net = AttU_Net(3,NUM_CLASSES).eval()\n",
    "\n",
    "\n",
    "            state_dict = torch.load(model_path, map_location='cpu')\n",
    "            net.load_state_dict(state_dict)\n",
    "\n",
    "            if cuda:\n",
    "                net = nn.DataParallel(net)\n",
    "                net = net.cuda()\n",
    "\n",
    "            print('{} model loaded.'.format(model_path))\n",
    "\n",
    "            if num_classes == 2:\n",
    "                colors = [(255, 255, 255), (0, 0, 0)]\n",
    "            elif num_classes == 3:\n",
    "                colors = [(255,255,255), (0, 0, 0), (128, 128, 128)]\n",
    "            elif num_classes <= 21:\n",
    "                colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
    "                                (0, 128, 128),\n",
    "                                (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), (192, 128, 0), (64, 0, 128),\n",
    "                                (192, 0, 128),\n",
    "                                (64, 128, 128), (192, 128, 128), (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0),\n",
    "                                (0, 64, 128), (128, 64, 12)]\n",
    "\n",
    "\n",
    "            def letterbox_image(image, size):\n",
    "                image = image.convert(\"RGB\")\n",
    "                iw, ih = image.size\n",
    "                w, h = size\n",
    "                scale = min(w / iw, h / ih)\n",
    "                nw = int(iw * scale)\n",
    "                nh = int(ih * scale)\n",
    "\n",
    "                image = image.resize((nw, nh), Image.BICUBIC)\n",
    "                new_image = Image.new('RGB', size, (128, 128, 128))\n",
    "                new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
    "                return new_image, nw, nh\n",
    "            \n",
    "\n",
    "            val_dataset = Test_DeeplabDataset(val_lines, inputs_size, NUM_CLASSES, False, dataset_path = '/openbayes/input/input3')\n",
    "            gen_val = DataLoader(val_dataset, batch_size=1, collate_fn=Test_deeplab_dataset_collate)\n",
    "            epoch_size_val = max(1, len(val_lines) // 1)\n",
    "\n",
    "            fit_one_epoch(net , gen_val, Cuda,filesname)\n",
    "\n",
    "    logger.info('finish testing!')\n",
    "    print(np.mean(background_dice_list))\n",
    "    print(np.mean(pet_dice_list))\n",
    "    print(np.mean(dice_list))\n",
    "    print(np.mean(sensitivity_list))\n",
    "    print(np.mean(precision_list))\n",
    "    print(np.mean(hd_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640773ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
